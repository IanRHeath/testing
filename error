Sending a simple test prompt to the LLM...

--- LLM Test FAILED ---
An error occurred while trying to invoke the LLM: Error code: 400 - {'elapsed': '00:00:00.1515704', 'llmService': 'AzureOpenAI', 'deploymentInfo': {'id': 'dvue-aoai-001-o3mini', 'version': '2024-12-01-preview'}, 'statusCode': 400, 'response': {'error': {'message': "Unsupported parameter: 'temperature' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_parameter'}}, 'message': 'Generate one or more predicted (chat) completions returned BadRequest.'}
This confirms the issue is with the LLM server or your connection to it.
Traceback (most recent call last):
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/test_llm_connection.py", line 21, in test_connection
    response = llm.invoke("Hello, are you working?")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 372, in invoke
    self.generate_prompt(
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 957, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 776, in generate
    self._generate_with_cache(
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 1022, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 995, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 925, in create
    return self._post(
           ^^^^^^^^^^^
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/users/iheath12/OneDrive - Advanced Micro Devices Inc/desktop/JiraTriageLLMAgent/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'elapsed': '00:00:00.1515704', 'llmService': 'AzureOpenAI', 'deploymentInfo': {'id': 'dvue-aoai-001-o3mini', 'version': '2024-12-01-preview'}, 'statusCode': 400, 'response': {'error': {'message': "Unsupported parameter: 'temperature' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_parameter'}}, 'message': 'Generate one or more predicted (chat) completions returned BadRequest.'}
